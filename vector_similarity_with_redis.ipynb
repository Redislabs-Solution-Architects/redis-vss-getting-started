{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bsbodden/redis_vss_getting_started/blob/main/vector_similarity_with_redis.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/bsbodden/redis_vss_getting_started/main/assets/nbviewer-shield.svg)](https://nbviewer.org/github/bsbodden/redis_vss_getting_started/blob/main/vector_similarity_with_redis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsTNahuVyEuH"
   },
   "source": [
    "# Similarity Search with Redis as a Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Unstructured Data\" Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, about 80% of the data organizations generate is \"unstructured\"; data that either does not have a well-defined schema or cannot be restructured into a familiar columnar format. Typical examples of unstructured data include free-form text, images, videos, and sound clips. Moreover, this data imbalance towards unstructured data is expected to grow in the coming decades.\n",
    "\n",
    "Unstructured data is high-dimensional and noisy,    making it more challenging to analyze and interpret using traditional methods. But it is also packed with information and meaning.  \n",
    "\n",
    "Traditionally, unstructured data is processed to extract specific features, effectively turning it into structured data. Once in the realm of structured data, we can search the data with SQL queries (if stored in a relational database) or with a text search engine. \n",
    "The approach of transforming unstructured data into structured data has a few issues; first, engineering features out of unstructured data can be computationally expensive and error-prone, significantly delaying when we can effectively use the data. And secondly, in the extractions/transformation process, lose fidelity and information since unique, 'latent' features are likely lost or overlooked because they can't be easily categorized or quantified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter \"Vector Databases\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach to dealing with unstructured data is to \"vectorize\" such data. By \"vectorizing,\" we mean to somehow convert something like a text passage, an image, a video, or a song into a flat sequence of numbers representing a particular piece of data. These \"vectors\" are representations of the data in N-dimensional space. By vectorizing, we gain the ability to use linear algebra techniques to compare, group, and operate on our data. This is the foundation of a Vector Database; the ability to store and operate on vectors.\n",
    "This approach is not new and has been around for eons. The difference today is how the techniques for generating the vectors have advanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning \"Embeddings\" as Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional methods for converting unstructured data into vector form include Bag-of-Words (BoW) and TF-IDF (Term Frequency-Inverse Document Frequency) for textual data. For categorical data, one-hot Encoding is a commonly used approach. And Hashing and Feature Extraction techniques, such as edge detection, texture analysis, or color histograms, have been employed for high-dimensionality data like images.\n",
    "\n",
    "While powerful in their own right, these approaches reveal limitations when confronted with high-dimensional and intricate data forms like long text passages, images, and audio. Consider, for example, how a text passage could be restructured—through sentence rearrangement, synonym usage, or alterations in narrative style. Such Simple modifications could effectively sidestep techniques like Bag-of-Words, preventing systems using the generated encodings from identifying texts with similar meanings.\n",
    "\n",
    "This is where advancements in Machine Learning, particularly Deep Learning, make their mark. Machine Learning models have facilitated the rise of 'embeddings' as a widely embraced method for generating dense, low-dimensional vector representations. Given a suitable model for the task at hand, the generated embeddings can encapsulate complex patterns and semantic meanings inherent in data, thus overcoming the limitations of their traditional counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Embeddings for the \"Bikes\" Dataset\n",
    "\n",
    "To investigate Vector Similarity, we'll use a subset of the Bikes dataset, a relatively simple synthetic dataset. The dataset has 11 bicycle records in a JSON file named `bikes.json` and includes the fields `model`, `brand`, `price`, `type`, `specs`, and `description`. The `description` field is particularly interesting for our purposes since it consists of a free-form textual description of a bicycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading JSON \"Bikes\" Dataset\n",
    "\n",
    "Let's load the bikes dataset as a JSON array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install required libraries\n",
    "!python3 -m pip -q install boto3 redis pandas matplotlib\n",
    "\n",
    "#%pip install -q redis pandas matplotlib\n",
    "#%pip -q install pandas\n",
    "#%pip install matplotlib\n",
    "\n",
    "#Install `sentence-transformers` library -- This is optional if you are using AWS Bedrock\n",
    "%pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/bsbodden/redis_vss_getting_started/main/data/bikes.json'\n",
    "response = requests.get(url)\n",
    "bikes = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the Bikes JSON\n",
    "\n",
    "Let's inspect the content of the JSON array in table form using the Pandas framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the structure of one of our bike JSON documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(bikes[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Text Embeddings using SentenceTransformers\n",
    "\n",
    "We will use the [SentenceTransformers](https://www.sbert.net/) framework to generate embeddings for the bikes descriptions. Sentence-BERT (SBERT) is a BERT model modification that produces consistent and contextually rich sentence embeddings. SBERT improves tasks like semantic search and text grouping by allowing for efficient and meaningful comparison of sentence-level semantic similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a suitable pre-trained Model\n",
    "\n",
    "We must pick a suitable model based on the task at hand when generating embeddings. In our case, we want to query for bicycles using short sentences against the longer bicycle descriptions. This is referred to as \"Asymmetric Semantic Search,\" often employed in cases where the search query and the documents being searched are of different nature or structure. Suitable models for asymmetric semantic search include pre-trained [MS MARCO](https://microsoft.github.io/msmarco/) Models. MS MARCO models are trained on the **M**icro**S**oft **MA**chine **R**eading **CO**mprehension dataset, and are optimized for understanding real-world queries and retrieving relevant responses. They are widely used in search engines, chatbots, and other AI applications. At the time of this writing, the highest performing MS MARCO model tuned for cosine-similarity available from SentenceTranformers is `msmarco-distilbert-base-v4`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "sample_description = bikes[0]['description']\n",
    "TextWrapper(width=120).wrap(sample_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Storing our bikes in Redis\n",
    "Now that we know how to vectorize the bikes descriptions, it's time to start working with Redis. We will use Redis Enterprise Cloud database to store our vectors. \n",
    "Note: In case Redis Enterprise is not available, use Redis Stack open source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment & execute the following code in case Redis Enterprise is not available\n",
    "# !curl -fsSL https://packages.redis.io/redis-stack/redis-stack-server-6.2.6-v7.focal.x86_64.tar.gz -o redis-stack-server.tar.gz\n",
    "# !tar -xvf redis-stack-server.tar.gz\n",
    "# !./redis-stack-server-6.2.6-v7/bin/redis-stack-server --daemonize yes\n",
    "#oss = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Note down the parameters required for Redis Enterprise Cloud like URL, password, port etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "host = 'redis-15315.c212.ap-south-1-1.ec2.cloud.redislabs.com'\n",
    "port = 15315\n",
    "password = 'admin'\n",
    "requirePass = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiwmgzpYJi8e"
   },
   "source": [
    "### Redis Python Client\n",
    "\n",
    "To interact with Redis, we'll install the [`redis-py`](https://github.com/redis/redis-py) client library, which encapsulates the commands to work with OSS Redis as well as Redis Stack:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJiYDtclJzvR"
   },
   "source": [
    "#### Create a `redis-py` client and test the server\n",
    "\n",
    "We'll instantiate the Redis client, connecting to the localhost on Redis' default port `6379`. By default, Redis returns binary responses; to decode them, we'll pass the `decode_responses` parameter set to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dssqMRpNzAkP",
    "outputId": "1bbd14a6-f890-46de-ed37-3ee412e9859a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "if requirePass:\n",
    "    client = redis.Redis(host = host, port=port, decode_responses=True, password=password)\n",
    "else:\n",
    "    client = redis.Redis(host = 'localhost', decode_responses=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Redis' [`PING`](https://redis.io/commands/ping/) command to check that Redis is up and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.ping()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Bikes as JSON Documents in Redis\n",
    "\n",
    "Redis Stack includes [JSON](https://redis.io/docs/stack/json/) functionality. Like any other Redis data type, the JSON datatype allows you to use Redis commands to save, update, and retrieve JSON values. \n",
    "Since we already have the bikes data loaded in memory as the `bikes` JSON array. We will iterate over `bikes`, generate a suitable Redis key and store them in Redis using the [`JSON.SET`](https://redis.io/commands/json.set/) command. We'll do this [pipeline](https://redis.io/docs/manual/pipelining/) mode to minimize the round-trip times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFVu3GM-VXma",
    "outputId": "058341a8-424c-40e1-e84c-6d2d3429661e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = client.pipeline()\n",
    "\n",
    "for i, bike in enumerate(bikes, start=1):\n",
    "    redis_key = f'bikes:{i:03}'\n",
    "    print(redis_key)\n",
    "    pipeline.json().set(redis_key, '$', bike)\n",
    "\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhREF_xwKqY3"
   },
   "source": [
    "Let's retrieve a specific value from one of the JSON bikes in Redis using a [JSONPath](https://goessner.net/articles/JsonPath/) expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "208hSDzxhvZw",
    "outputId": "f42afd46-06e4-4458-9fba-d103ee389782",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.json().get('bikes:010', '$.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIlXb-Y2QcVb"
   },
   "source": [
    "### Vectorize all of the Bikes Descriptions\n",
    "\n",
    "To vectorize all the descriptions in the database, we will first collect all the Redis keys for the bikes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the keys as a parameter to the [`JSON.MGET`](https://redis.io/commands/json.mget/) command, along with the JSONPath expression `$.description` to collect the descriptions in a list which we will then pass to the `encode` method to get a list of vectorized embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juIELWOeIl7T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we will first collect all the Redis keys for the bikes\n",
    "keys = sorted(client.keys('bikes:*'))\n",
    "\n",
    "# Next will accumulate all the descriptions in List format\n",
    "descriptions = client.json().mget(keys, '$.description')\n",
    "descriptions = [item for sublist in descriptions for item in sublist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## To create the vector embeddings we can leverage:\n",
    "1. Recommended: Use AWS Bedrock API and invoke Titan embedding model\n",
    "2. Another option: Use the SentenceTransformers framework to generate embeddings for the bikes descriptions.\n",
    "   Sentence-BERT (SBERT) is a BERT model modification that produces consistent and contextually rich sentence embeddings. \n",
    "   SBERT improves tasks like semantic search and text grouping by allowing for efficient and meaningful comparison of \n",
    "   sentence-level semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Embedding using Bedrock API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDgOYEAeQj_C"
   },
   "source": [
    "Now we can add the vectorized descriptions to the JSON documents in Redis using the `JSON.SET` command to insert a new field in each of the documents under the JSONPath `$.description_embeddings`, once again we'll do this in pipeline mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "#bedrock2 = boto3.client('bedrock')\n",
    "\n",
    "# This method that generates embedding using Bedrock API with Titan embedding model\n",
    "def generate_embedding(body):\n",
    "    modelId = 'amazon.titan-e1t-medium'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    \n",
    "    bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "    #bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's check if Bedrock embeddings API are working well and take a peek at the first 5 elements of the generated vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\"inputText\": sample_description})\n",
    "embedding = generate_embedding(body)\n",
    "print(embedding[:5])\n",
    "\n",
    "# Also check the dimension of the model\n",
    "VECTOR_DIMENSION = len(embedding)\n",
    "VECTOR_DIMENSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate & store the embeddings \n",
    "If everything is fine at this point, we can proceed further.\n",
    "Here we will do 2 things:\n",
    "1. Generate the embeddings of 'description' field present in the JSON object.\n",
    "2. And finally storing the embeddings in a new element 'description_embeddings' inside the same JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with redis server\n",
    "pipeline = client.pipeline()\n",
    "\n",
    "for key, description in zip(keys, descriptions):\n",
    "    input_query = json.dumps({\"inputText\": description})\n",
    "    embedding = generate_embedding(body=input_query)\n",
    "    pipeline.json().set(key, '$.description_embeddings', embedding)\n",
    "\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Embedding using SentenceTransformers BERT API\n",
    "If option 1 is completed successfully, skip Option 2 and proceed to the next section.\n",
    "However, if you have issues in following Option 1 (For e.g, due to any issues related to Bedrock APIs like outage, throttling, rate-limiting etc), \n",
    "you can follow this option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "\n",
    "embedding = embedder.encode(sample_description)\n",
    "body = json.dumps({\"inputText\": sample_description})\n",
    "embedding = generate_embedding(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if SentenceTransformer embeddings API are working well and take a peek at the first 5 elements of the generated vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedder.encode(sample_description)\n",
    "print(embedding[:5])\n",
    "\n",
    "# Also check the dimension of the model\n",
    "VECTOR_DIMENSION = len(embedding)\n",
    "VECTOR_DIMENSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate & store the embeddings \n",
    "If everything is fine at this point, we can proceed further.\n",
    "Here we will do 2 things:\n",
    "1. Generate the embeddings of 'description' field present in the JSON object.\n",
    "2. And finally storing the embeddings in a new element 'description_embeddings' inside the same JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EQpUGSdDzX8",
    "outputId": "744befe8-5260-46f8-ad2d-4c523ae16c1b"
   },
   "outputs": [],
   "source": [
    "pipeline = client.pipeline()\n",
    "\n",
    "for key, embedding in zip(keys, embeddings):\n",
    "    pipeline.json().set(key, '$.description_embeddings', embedding)\n",
    "\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the data in Redis\n",
    "Let's inspect one of the vectorized bike documents using the `JSON.GET` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNmpOjy3OWM3",
    "outputId": "d365f641-8de6-4773-dda5-70f971436514",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(client.json().get('bikes:010'), indent=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When storing a vector embedding as part of a JSON datatype, the embedding is stored as a JSON array, in our case, under the field `description_embeddings` as shown.\n",
    "\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RctF2ngSQ5wV"
   },
   "source": [
    "### Making the bikes collection searchable\n",
    "\n",
    "Redis Stack provides a powerful search engine ([RediSearch](https://redis.io/docs/stack/search/)) that introduces [commands](https://redis.io/docs/stack/search/commands/) to create and maintain search indices for both collections of HASHES and [JSON](https://redis.io/docs/stack/search/indexing_json/) documents.\n",
    "\n",
    "To create a search index for the bikes collection, we'll use the [`FT.CREATE`](https://redis.io/commands/ft.create/) command:\n",
    "\n",
    "```\n",
    "1️⃣ FT.CREATE idx:bikes_vss ON JSON \n",
    "2️⃣  PREFIX 1 bikes: SCORE 1.0 \n",
    "3️⃣  SCHEMA \n",
    "4️⃣    $.model TEXT WEIGHT 1.0 NOSTEM \n",
    "5️⃣    $.brand TEXT WEIGHT 1.0 NOSTEM \n",
    "6️⃣    $.price NUMERIC \n",
    "7️⃣    $.type TAG SEPARATOR \",\" \n",
    "8️⃣    $.description AS description TEXT WEIGHT 1.0 \n",
    "9️⃣    $.description_embeddings AS vector VECTOR FLAT 6 TYPE FLOAT32 DIM 768 DISTANCE_METRIC COSINE\n",
    "```\n",
    "\n",
    "There is a lot to unpack here; let's take it from the top:\n",
    "\n",
    "- 1️⃣ We start by specifying the name of the index; `idx:bikes` indexing keys of type `JSON`.\n",
    "- 2️⃣ The keys being indexed are found using the `bikes:` key prefix.\n",
    "- 3️⃣ The `SCHEMA` keyword marks the beginning of the schema field definitions.\n",
    "- 4️⃣ Declares that field in the JSON document at the JSONPath `$.model` will be indexed as a `TEXT` field, allowing full-text search queries (disabling stemming).\n",
    "- 5️⃣ The `$.brand` field will also be treated as a `TEXT` schema field.\n",
    "- 6️⃣ The `$.price` field will be indexed as a `NUMERIC` allowing numeric range queries.\n",
    "- 7️⃣ The `$.type` field will be indexed as a `TAG` field. Tag fields allow exact-match queries, and are suitable for categorical values.\n",
    "- 8️⃣ The `$.description` field will also be indexed as a `TEXT` field.\n",
    "- 9️⃣ Finally, the vector embeddings in `$.description_embeddings` are indexed as a `VECTOR` field and assigned to the alias `vector`. \n",
    "  \n",
    "Let's break down the `VECTOR` schema field definition to better understand the inner workings of Vector Similarity in Redis:\n",
    "\n",
    "* `FLAT`: Specifies the indexing method, which can be `FLAT` or `HNSW`. FLAT (brute-force indexing) provides exact results but at a higher computational cost, while HNSW (Hierarchical Navigable Small World) is a more efficient method that provides approximate results with lower computational overhead.\n",
    "* `TYPE`: Set to `FLOAT32`. Current supported types are `FLOAT32` and `FLOAT64`.\n",
    "* `DIM`: The length or dimension of our embeddings, which we determined previosly to be `768`.\n",
    "* `DISTANCE_METRIC`: One of `L2`, `IP`, `COSINE`. \n",
    "  - `L2` stands for \"Euclidean distance\"; a straight-line distance between the vectors. Preferred when the absolute differences, including magnitude, matter most.\n",
    "  - `IP` stands for \"Inner Product\"; IP measures the projection of one vector onto another. It emphasizes the angle between vectors rather than their absolute positions in the vector space.\n",
    "  - `COSINE` stands for \"Cosine Similarity\"; a normalized form of inner product. This metric measures only the angle between two vectors, making it magnitude-independent.  \n",
    "  - For our querying purposes, the direction of the vectors carry more meaning (indicating semantic similarity), and the magnitude is largely influenced by the length of the documents, therefore `COSINE` similarity is chosen. Also, our chosen embedding model is fine-tuned for Cosine Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python code below creates the Redis Search Index for the bikes collection equivalent to the previous raw `FT.CREATE` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5i08gIFOugP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from redis.commands.search.field import TagField, TextField, NumericField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "\n",
    "INDEX_NAME = 'idx:bikes_vss'\n",
    "DOC_PREFIX = 'bikes:'\n",
    "\n",
    "try:\n",
    "    # check to see if index exists\n",
    "    client.ft(INDEX_NAME).info()\n",
    "    print('Index already exists!')\n",
    "except:\n",
    "    # schema\n",
    "    schema = (\n",
    "        TextField('$.model', no_stem=True, as_name='model'),\n",
    "        TextField('$.brand', no_stem=True, as_name='brand'),\n",
    "        NumericField('$.price', as_name='price'),\n",
    "        TagField('$.type', as_name='type'),\n",
    "        TextField('$.description', as_name='description'),\n",
    "        VectorField('$.description_embeddings',\n",
    "            'FLAT', {\n",
    "                'TYPE': 'FLOAT32',\n",
    "                'DIM': VECTOR_DIMENSION,\n",
    "                'DISTANCE_METRIC': 'COSINE',\n",
    "            },  as_name='vector'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # index Definition\n",
    "    definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.JSON)\n",
    "\n",
    "    # create Index\n",
    "    client.ft(INDEX_NAME).create_index(fields=schema, definition=definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubBHtW_TRBmW"
   },
   "source": [
    "#### Check the state of the index\n",
    "\n",
    "After the `FT.CREATE` creates the index, the indexing process is automatically started in the background. In the blink of an eye, our 11 JSON documents should be indexed and ready to be searched. To corroborate that, we use the [`FT.INFO`](https://redis.io/commands/ft.info/) command to check some information and statistics on the index. Of particular interest are the number of documents successfully indexed, and the number of failures:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN80EZohP--J",
    "outputId": "f4b2dedf-e285-4041-c77d-e78939a806bb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "info = client.ft(INDEX_NAME).info()\n",
    "\n",
    "num_docs = info['num_docs']\n",
    "indexing_failures = info['hash_indexing_failures']\n",
    "total_indexing_time = info['total_indexing_time']\n",
    "percent_indexed = int(info['percent_indexed']) * 100\n",
    "\n",
    "\n",
    "print(f\"{num_docs} documents ({percent_indexed} percent) indexed with {indexing_failures} failures in {float(total_indexing_time):.2f} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Data Searches with Redis\n",
    "\n",
    "The index `idx:bikes_vss` indexes the structured fields of our JSON documents `model`, `brand`, `price`, and `type`. It also indexes the unstructured free-form text `description` and the generated embeddings in `description_embeddings`. Before we dive deeper into Vector Similarity Search, we need to understand the basics of querying a Redis index. The Redis command of interest is [`FT.SEARCH`](https://redis.io/commands/ft.search/). Like a SQL select statement, an `FT.SEARCH` invocation can be as simple or as complex as needed. \n",
    "\n",
    "Let's try simple queries that give enough context to complete our VSS examples. For example, to retrieve all bikes where the `brand` is `Peaknetic`, we can use the following command:\n",
    "\n",
    "```\n",
    "FT.SEARCH idx:bikes_vss '@brand:Peaknetic'\n",
    "```\n",
    "\n",
    "The command will return all matching documents. With the inclusion of the vector embeddings, that's a little too verbose. If we wanted to only return specific fields from our JSON documents, for example, the document `id`, the `brand`, `model` and `price`, we could use:\n",
    "\n",
    "```\n",
    "FT.SEARCH idx:bikes_vss '@brand:Peaknetic' RETURN 4 id, brand, model, price\n",
    "```\n",
    "\n",
    "In the query, we are searching against a schema field of type `TEXT`. The equivalent Python code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('@brand:Peaknetic').return_fields('id', 'brand', 'model', 'price')\n",
    ")\n",
    "client.ft(INDEX_NAME).search(query).docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we only wanted bikes under `$1000`. We can add a numeric range clause to our query since the `price` field is indexed as `NUMERIC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('@brand:Peaknetic @price:[0 1000]').return_fields('id', 'brand', 'model', 'price')\n",
    ")\n",
    "client.ft(INDEX_NAME).search(query).docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf6KLjOqRJ07"
   },
   "source": [
    "### Semantic Searching with Vector Similarity Search\n",
    "\n",
    "Now that the bikes collection is stored and properly indexed in Redis, we want to query them using short query prompts. Let's put our queries in a list so we can execute them in bulk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU3rPCCIOfNc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'Bike for small kids',\n",
    "    'Best Mountain bikes for kids',\n",
    "    'Cheap Mountain bike for kids',\n",
    "    'Female specific mountain bike',\n",
    "    'Road bike for beginners',\n",
    "    'Commuter bike for people over 60',\n",
    "    'Comfortable commuter bike',\n",
    "    'Good bike for college students',\n",
    "    'Mountain bike for beginners',\n",
    "    'Vintage bike',\n",
    "    'Comfortable city bike'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the query prompts to query the database using VSS. Just like we did with the descriptions of the bikes, we'll use the SentenceTransformers model to encode the queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQlt6HGgUQut",
    "outputId": "b723809d-a9bd-4eb1-94e2-b786bbb9b039",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encoded_queries = embedder.encode(queries)\n",
    "#len(encoded_queries)\n",
    "encoded_queries = []\n",
    "for query in queries:\n",
    "    input_query = json.dumps({\"inputText\": query})\n",
    "    query_embedding = generate_embedding(body=input_query)\n",
    "    encoded_queries.append(query_embedding)\n",
    "\n",
    "len(encoded_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOLqqTBERO-G"
   },
   "source": [
    "#### Constructing a \"Pure KNN\" VSS Query\n",
    "\n",
    "We'll start with a K-nearest neighbors (KNN) query. KNN is a foundational algorithm used in vector similarity search, where the goal is to find the most similar items to a given query item. Using the chosen distance metric, the KNN algorithm calculates the distance between the query vector and each vector in the database. It then returns the 'K' items with the smallest distances to the query vector. These are considered to be the most similar items.\n",
    "\n",
    "The syntax for vector similarity KNN queries is `(*)=>[vector_similarity_query>]` where the `(*)` (the `*` meaning all) is the filter query for the search engine. That way, one can reduce the search space by filtering the collection on which the KNN algorithm operates. \n",
    "\n",
    "* The `$query_vector` represents the query parameter we'll use to pass the vectorized query prompt.\n",
    "* The results will be filtered by `vector_score`, which is a field derived from the name of the field indexed as a Vector by appending `_score` to it, in our case, `vector` (the alias for `$.description_embeddings`). \n",
    "* Our query will return the `vector_score`, the `id` of the match documents, and the `$.brand`, `$.model`, and `$.description`. \n",
    "* Finally, to utilize a vector similarity query with the `FT.SEARCH` command, we must specify DIALECT 2 or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('(*)=>[KNN 3 @vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'id', 'brand', 'model', 'description')\n",
    "     .dialect(2)\n",
    ")\n",
    "\n",
    "client.ft(INDEX_NAME).search(query, { 'query_vector': np.array(encoded_queries[0], dtype=np.float32).tobytes() }).docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the vectorized query as `$query_vector` to the search function to execute the query. The following code shows an example of creating a NumPy array from a vectorized query prompt (`encoded_query`) as a single precision float array and converting it into a compact, byte-level representation that we can pass as a Redis parameter:\n",
    "\n",
    "```python .noeval\n",
    "client.ft(INDEX_NAME).search(query, { 'query_vector': np.array(encoded_query, dtype=np.float32).tobytes() }).docs\n",
    "````\n",
    "\n",
    "With the template for the query in place, we can use a bit of Python code to execute all query prompts in a loop, passing the vectorized query prompts. Notice that for each result we calculate the `vector_score` as `1 - doc.vector_score`, since we use cosine \"distance\" as the metric, the items with the smallest \"distance\" are closer and therefore more similar to our query. \n",
    "\n",
    "We will then loop over the matched documents and create a list of results that will help up make a lovely Pandas table to visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_query_table(query, queries, encoded_queries, extra_params = {}):\n",
    "    results_list = []\n",
    "    for i, encoded_query in enumerate(encoded_queries):\n",
    "        result_docs = client.ft(INDEX_NAME).search(query, { 'query_vector': np.array(encoded_query, dtype=np.float32).tobytes() } | extra_params).docs\n",
    "        for doc in result_docs:\n",
    "            vector_score = round(1 - float(doc.vector_score), 2)\n",
    "            results_list.append({\n",
    "                'query': queries[i], \n",
    "                'score': vector_score, \n",
    "                'id': doc.id,\n",
    "                'brand': doc.brand,\n",
    "                'model': doc.model,\n",
    "                'description': doc.description\n",
    "            })\n",
    "\n",
    "    # Pretty-print the table\n",
    "    queries_table = pd.DataFrame(results_list)\n",
    "    queries_table.sort_values(by=['query', 'score'], ascending=[True, False], inplace=True)\n",
    "    queries_table['query'] = queries_table.groupby('query')['query'].transform(lambda x: [x.iloc[0]] + ['']*(len(x)-1))\n",
    "    queries_table['description'] = queries_table['description'].apply(lambda x: (x[:497] + '...') if len(x) > 500 else x)\n",
    "    html = queries_table.to_html(index=False)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query results show the individual queries' top 3 matches (our K parameter) along with the bike's id, brand, and model for each query. For example, for the query  \"Best Mountain bikes for kids\", the highest similarity score (`0.54`) and therefore the closest match was the 'Nord' brand 'Chook air 5' bike model, described as:\n",
    "\n",
    "> \"The Chook Air 5 gives kids aged six years and older a durable and uberlight mountain bike for their first experience on tracks and easy cruising through forests and fields. The lower top tube makes it easy to mount and dismount in any situation, giving your kids greater safety on the trails. The Chook Air 5 is the perfect intro to mountain biking.\"\n",
    "\n",
    "From the description, we gather that this bike is an excellent match for younger children, and the MS MARCO model-generated embeddings seem to have captured the semantics of the description accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_query_table(query, queries, encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes \"a picture is worth a thousand words\". Using the dimensionality reduction technique [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) we can create a 3-d representation of our description embeddings, as well as the query embeddings which shows how well the MS Marco Sentence Embeddings clustered our bicycle descriptions and we can visually judge a query's distance to a specific bike:\n",
    "\n",
    "![t-SNE 3-D Embeddings Visualization](https://raw.githubusercontent.com/bsbodden/redis_vss_getting_started/3ac967dfbdd84dad25ade620826c0e01ac0251ca/embeddings-tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Queries\n",
    "\n",
    "\"Pure KNN\" queries, as in the previous section, evaluate a query against the whole space of vectors in a data collection. The larger the collection, the more computationally expensive the nearest neighbors' search will be, but in the real world, unstructured data does not live in isolation, and users expecting rich search experiences need to be able to search via a combination of structured and unstructured data. \n",
    "\n",
    "For example, users might arrive at your search interface with a brand preference in mind for the bikes dataset. Redis VSS queries can use this information to pre-filter the search space using a \"primary filter query\". In the following query definition, we pre-filter using the `brand` to consider only `Peaknetic` brand bikes. Notice that, before our primary filter query was `(*)`, AKA everything, but now we can narrow the search space using `(@brand:Peaknetic)` before the KNN query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hybrid_query = (\n",
    "    Query('(@brand:Peaknetic)=>[KNN 3 @vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'id', 'brand', 'model', 'description')\n",
    "     .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering by the `Peaknetic` brand, for which there are 2 bikes in our collection, we can see the results returned for each of the query prompts. The query with the highest returned similarity score is \"Comfortable commuter bike\", followed by \"Road bike for beginners\". By filtering by brand, we fulfill the users' preferences and reduce the KNN search space by %80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_query_table(hybrid_query, queries, encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a VSS Range Query \n",
    "\n",
    "Range queries in Vector Similarity Search (VSS) involve retrieving items within a specific distance from a query vector. In this case, we consider \"distance\" to be the measure of similarity we've used to build our search indices; the smaller the distance, the more similar the items.\n",
    "\n",
    "Let's say you want to find the bikes whose descriptions are within a certain distance from a query vector. We can use a range query to achieve this.\n",
    "For example, the query command to return the top `4` documents within a `0.55` radius of a vectorized query would be as follows: \n",
    "\n",
    "```\n",
    "1️⃣ FT.SEARCH idx:bikes_vss \n",
    "2️⃣   @vector:[VECTOR_RANGE $range $query_vector]=>{$YIELD_DISTANCE_AS: vector_score} \n",
    "3️⃣   SORTBY vector_score ASC\n",
    "4️⃣   LIMIT 0 4 \n",
    "5️⃣   DIALECT 2 \n",
    "6️⃣   PARAMS 4 range 0.55 query_vector \"\\x9d|\\x99>bV#\\xbfm\\x86\\x8a\\xbd\\xa7~$?*....\"\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- 1️⃣ We use the `FT.SEARCH` command with our `idx:bikes_vss`.\n",
    "- 2️⃣ and filter by the `vector` using the `VECTOR_RANGE` operator pasing the `$range` parameter, yield the vector distance between the vector field and the query result in a field named `vector_score`.\n",
    "- 3️⃣ We sort the results by the yielded `vector_score`.\n",
    "- 4️⃣ Limit the results to at most 4.\n",
    "- 5️⃣ Once again, we set the RediSearch dialect to `2` to enable VSS functionality.\n",
    "- 6️⃣ Finally we set the parameter values, `range` (`$range`) to `0.55` and the `query_vector` (`$query_vector`) to the encoded vectorized query. \n",
    "\n",
    "The equivalent Python query definition is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "range_query = (\n",
    "    Query('@vector:[VECTOR_RANGE $range $query_vector]=>{$YIELD_DISTANCE_AS: vector_score}') \n",
    "    .sort_by('vector_score')\n",
    "    .return_fields('vector_score', 'id', 'brand', 'model', 'description')\n",
    "    .paging(0, 4)\n",
    "    .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the first query prompt in our collection of queries, \"Bike for small kids\", using the VSS range query defined (`range_query`). We can use the `create_query_table` utility function to execute the query passing the extra parameter `$range` as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_query_table(range_query, queries[:1], encoded_queries[:1], {'range': 0.55})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query returns two bikes in the specified range of our vectorized query, both with scores at or below `0.55`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "In this guide, we learned how Redis, using the Redis Stack distribution, provides powerful search capabilities over structured and unstructured data. Redis support for vector data can enrich and enhance the user's search experience.\n",
    "Although we focused on generating embeddings for unstructured data, the vector similarity approach can equally be employed with structure data, as long as a suitable vector generation technique is used.\n",
    "\n",
    "The references below can help you learn more about Redis search capabilities:\n",
    "* https://redis.io/docs/stack/search/\n",
    "* https://redis.io/docs/stack/search/indexing_json/"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
